1.信息过载
社会信息超过了个人或系统所能接受、处理货有效利用的范围。并导致故障的情况。

2.信息检索定义
分类目录是将网站信息系统地分类整理，搜索引擎是指自动从英特网岁哦及信息，经过一定整理后，提供给用户进行查询的系统
常用的搜索引擎是Web搜索，是信息检索的一个分支，学术上的信息检索（Information Retrieval 简称 IR）：信息检索式从大规模非结构化数据的集合中找出满足用户信息需求的资料的过程

3.信息检索常用术语
用户需求：UN
查询：Query，UN提交给检索系统时称为查询
文档：Document 文档是信息检索的对象
文档集：Crops 由若干文档构成的集合称为文档集合，文档集有时称为语料库
文档编号：DocumentID，给文档集中每个文档赋予的唯一标识符，通过文档ID来区分不同的文档
词条化：tokenization，将给定的字符序列拆分成一系列子序列的过程，拆分的每个子序列称为一个词条。词条化的过程可能会丢失标点符号或特殊字符。
词项：Term 经过语言学预处理之后归一化的词条，词项是索引的最小单位一般情况下把词项当做词，但词项不一定就是词。
词项-文档关联矩阵：Incidence matrix 标识词项和文档之间所具有的的一种包含关系的概念模型，每列代表一个文档，每行代表一个词项。
从纵向即文档这个维度看，每列代表一个文档包含词项信息。从横向即词项这个维度看，每行代表该词项在文档中的分布信息。
词项频率：Term frequency 同一个单词在某个文档中出现的频率
文档频率：Document frequency 出现某词项的文档的数目（某个词项在几个文档中出现的次数）
倒排记录表：Postings lists 用于记录出现过某个单词的所有文档的文档列表以及单词在该文档中线的位置信息，每条记录称为一个倒排项
倒排文件：Inverted file 倒排记录表在磁盘中的物理存储文件称为倒排文件

1.1.4 信息检索系统
分为 信息采集、信息整理、用户查询3部分
1.信息采集：通常是网络爬虫（Spider）
2.信息整理：按照规则进行编排，信息检索系统整理信息的过程称为索引构建，不仅要保存搜集起来的信息，好药将他们按照一定的规则进行编排。
3.接受查询：用户向信息检索系统发出查询请求，向用户返回检索到的文档，按照每个用户的要求检查自己的索引。

1.2 分词算法
1.2.1 分词算法概述
词是表达语义最小的单位，分词的质量影响了搜索结果的精准度，分词在文本索引的建立过程和用户提交检索过程中都存在。
1.英文分词原理：输入文本，词汇分割，词汇过滤，词干提取，大写转换小写，结果输出。
2.中文分词原理：主要有三种方法：基于词典匹配的分词方法、基于语义理解的分词、基于词频统计的分词

1.2.2 词典匹配分词法
按照一定的匹配策略将输入的字符串域机器字典词条进行匹配，把一个句子从左到右扫描以便，遇到字典中有的词就表示出来，遇到复合词就找到最长的词匹配，遇到不认识的子串则切分成单个词。
中文分词最大的问题是歧义处理，结合中文语言自身的特点，采用逆向匹配的切分算法，处理的精度高于正向匹配，产生的歧义现象也比较少。

1.2.3 语义理解分词法
模拟人脑对语言和句子的理解，达到识别词汇单元的效果。基本模式是把分词，句法，语义分析并进行，利用句法和语义信息来处理分词的歧义。基于语义理解的分词方法需要使用大量的语言知识和信息

1.2.4 词频统计分词法
基于人们对中文词语的直接感觉，词是稳定的字的组合，相邻的字搭配出现的频率越多，就越有可能形成一个固定的词。字与字相邻同时出现的频率或概率能够较好的反映词的可信度。当紧密程度高于某一个阈值时，便可以认为此字组可能构成一个词。
这种做法通常需要抽出一些共现频率高但不是词的常用字组，需要专门处理，提高精度。

1.3 倒排索引
Inverted index 也称为反向索引，是一种索引方法，被用来储存在全文搜索下某个单词在一个文档或一组文档中的存储位置的映射，是文档检索中最常用的数据结构。
其结构类似于： 从文档包含单词到单词所属文档的转换，就是倒排的由来（词语，文档 两个列 就是表示一个词语在那些文档中出现了。）
使用过程：先要经过词条化处理，文档频率也是倒排记录表的长度。

1.4 布尔检索模型
检索模型是判断文档内容与用户查询相关性的核心技术。
布尔检索模型是利用布尔运算符连接各个检索词，然后由计算机进行逻辑运算，找出所需信息的一种检索方法。
布尔检索模型中主要有 AND，OR，NOT 三种逻辑运算，作用是把检索词连接起来，构成一个逻辑检索式。
运算符之间的优先级：NOT->AND->OR 可以利用小括号“()” 设置个性化的检索方程。
例：(university OR college) AND (education OR Law) NOT Japan
单词-文档矩阵 从行来看，每一行是一个行向量，对应每个词项的文档向量，表示该词项在那些文档中出现，在那些文档中不出现。
从列来看，每一列是一个列向量，对应每个文档的词向量，表示干文档中那些词项出现了，那些词项没有出现。
在使用 布尔检索方式时，取出对应的行向量，进行计算
例：谷歌：0 1 0 1  开元：0 1 0 1 大会：1 0 0 0 （取反后：0 1 1 1） 计算：谷歌 AND 开源 NOT 大会 
运算后结果：0 1 0 1 ，劫镖表示（列）文档2,4 符合搜索结果

优点：与人们的思维习惯一致，布尔表达式表达直观清晰，方便用户进行扩检和缩检，易于计算机实现。
缺点：检索策略只基于0,1 二元判定标准，没有反应概念之间内在的语义联系，完全匹配会导致太少的结果文档被返回。

1.5 tf-idf 权重计算（略）

1.6 向量空间模型 （略）

1.7 概率检索模型（略）


第二章 Lucene开发入门

2.1 Lucene概述

2.1.1 Lucene简介
2.1.2 Lucene特点
灵活的切面，高亮，Join 和 group by 功能

2.1.3 Lucene架构
首先是信息采集的过程，完成信息采集后到 Lucene层面主要有两大任务，索引文档和搜索文档，应用层的第三部分就是用户结构，Lucene完成文档搜索任务，经过分词，匹配，评分，排序等一系列过程之后返回用户想要的文档。
第一步：查询分析（包含纠错）
第二步：分词技术（可以有多种分词方法），利用自然语言处理技术将用户输入的查询语句进行分词。
第三步：关键词检索 采用倒排索引的方式，在倒排索引库中进行匹配，倒排索引就是关键词和文档之间的对应关系。
第四步：搜索排序 排序处理后，返回检索结果。

2.2 Lucene开发准备

2.2.1 下载 Lucene文件库
Lucene是一个做全文检索的库，可以拿来根据实际业务需求进行使用。 从  lucene.apache.org 进行下载

2.2.2 工程中引入 Lucene
有两种方式，一种是传统的手工导入 jar包，核心包：lucene-6.0.0/core/lucene-core-6.0.0.jar
第二种是使用 maven管理 

2.2.3 下载 Luke
数据被处理为索引结构，采用 Luke查看Lucene，Solr，Elasticsearch索引的 GUI工具。其中 Luke的版本要与 Lucene的版本一致。
下载地址在：https://github.com/Dmitrykey/luke/releases 中下载

2.2.4 下载 IK分词工具
开源的，基于 Java语言的轻量级中文分词工具包，3.0后发展为面向Java的公共分析组件，独立于Lucene项目，
采用特有的“正向迭代最细粒度切分算法”，支持细粒度的只能分析两种切分模式。
下载地址：https://code.google.com/archive/p/ik-analyzer/downloads 下载使用


2.3 Lucene分词详解
2.3.1 Lucene分析系统  索引和查询都是以词频为基本单位，主要依靠 Analyzer类解析实现，Analyzer是一个抽象类，切分词的具体规则是由子类实现的。
Analyzer内部通过 TokenStream类实现，Tonkernizer类和TokenFilter类是 TokenStream的两个子类。Tokenizer处理单个字符组成的字符流，读取Reader对象中的数据，处理后转换成词汇单元。TokenFilter完成文本过滤器的功能，在使用过程中必须注意不同过滤器的使用顺序。
在创建索引的时候，也会用到分词器，进行索引查询的时候也会用到分词器。Lucene提供了多种分词方法。

1.StopAnalyzer（停用分词器），过滤词汇中的特定字符串和词汇
2.SAtandardAnalyzer（标准分词器），根据空格和符号来完成分词
3.WhitespaceAnalyzer（空格分词器），使用空格作为间隔符的词汇分割分词器
4.SimpleAnalyzer（简单分词），以非字母字符作为分隔符号
5.CJKAnalyzer（二分法分词），对中文进行分词，实现中文的多元切分和停用词过滤
6.KeywordAnalyzer（关键词分词），把整个输入作为一个单独词汇单元，方便特殊类型的文本进行索引和检索

2.3.2 分词器测试（代码）

2.3.3 IK分词器配置（代码）

2.2.4 中文分词器对比（代码）

2.3.5 扩展停用词分词器
配置停用器词典，默认是 stopword.dic 扩展可以到 https://github.com/cseryp/stopwords 下载作战停用词词表
在 IKAnalyzer.cfg.xml专用配置 <entry key="ext_stopwords">stopword.dic;ext_stopword.dic</entry>

2.3.6 扩展自定义词典
配置自定义词典，在同一目录新建 xxx.dic，然后编辑 IKAnalyzer.cfg.xml 进行配置 <entry key="ext_dict">ext.dic;</entry>


2.4 Lucene索引详解
索引文档就是把文档变成索引这种数据结构的过程：

2.4.1 Lucene字段类型
1.TextField 把该字段的内容索引并词条化，但不保存词向量。
2.StringField 只会对该字段的内容索引，但是并不词条化，也不保存词向量。字符串的值会被索引为一个单独的词项。
3.IntPoint 适合索引值为 int类型的字段，为了快速过滤的。
4.LongPoint 适合索引值为长整型 long类型的字段
5.FloatPoint 适合索引值为 float类型的字段
6.DoublePoint 适合索引值为 double类型的字段
7.SortedDocValuesField 存储值为文本内容的 DocValue字段，适合索引字段值为文本内容并且需要按值进行排序的字段
8.SortedSetDocValuesField 存储多值域的 DocValues字段，适合索引字段值为文本内容并且需要按值进行分组，聚合等操作的字段
9.NumericDocValuesField 存储单个数值类型的DocValues字段，
10.SortedNumericDocValuesField 存储数值类型的有序数组列表的 DocValues字段
11.StoredField 只排序字段，适合索引只需要保存字段值不进行其他操作的字段

DocValues是 Lucene4.X 版本以后新增的重要特性，是Lucene在构建索引时额外建立一个有序的基于 document=>field/value的映射列表。
在构建索引时会对开启 docvalues的字段额外构建一个已经排好序的文档到字段级别的一个列式存储映射，减轻了在排序和分组时对内存的依赖。

2.4.2 索引文档示例
Lucene索引文档要依靠一个 IndexWriter对象，该对象的 addDocument方法用于添加文档，最后调用 
文档是 Lucene索引和搜索的基本单位，比文档更小的单位是域，也可以称为字段，一个文档可以有多个域。FieldType对象用于指定域的索引信息。
FieldType对象的 setIndexOptions方法可以设定域的索引选项。（FieldType用来设置索引字段的信息，不同的参数设定不同的配置，索引的内容不同）
1.IndexOptions.DOCS 其中DOCS表示文档，只索引文档，词项频率和位移信息不保存
2.IndexOptions.DOCS_AND_FREQS 只索引文档和词频率信息，EREQS表示词频
3.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS 索引文档，词项频率和位移信息，POSITIONS表示位移信息
4.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS 索引文档，词项频率，位移信息和偏移量  OFFSETS表示偏移量
5.IndexOptions.NONE 不索引

全文检索中很重要的一项需求是关键字高亮，想准确获取位置信息以及一些偏移量就需要在创建索引的时候进行记录。在索引的时候，可以使用 FieldType对象提供的方法设置相对增量和位移信息。（信息修正）
1.setStored 是否存储字段
2.setTokenized 设置是否使用配置的分词器对字段进行词条化
3.setStoreTermVectors 设置词向量
4.setStoreTermVectorPositions 设置词项在词向量中的位移信息
5.setStoreTermVectorOffsets 设置词项在词向量中的偏移信息

