2.5.3 词项搜索（TermQuery）
在搜索引擎中，最基本的搜索就是在索引中搜索某一词条，词条是最基本的搜索单位。 
词条就是一个 key/value对。key是字段名，value则表示字段中所包含的某个关键字。
Term term = new Term("title","美国");
Query termQuery = new TermQuery(term);

2.5.4 布尔搜索（BooleanQuery）
是一个组合的Query，把各种 Query添加进去，表明之间的逻辑关系。提供了专门的API，往其添加子句，BooleanClause对象可以指定查询的包含关系，并作为参数通过 BooleanQuery.Builder()构造布尔查询。
Query query1 = new TermQuery(new Term("title","美国"))；
Query query2 = new TermQuery(new Term("content","日本"))
BooleanClause bc1 = new BooleanClause(query1, BooleanClause.Occur.MUST);
BooleanClause bc2 = new BooleanClause(query2, BooleanClause.Occur.MUST_NOT);
BooleanQuery boolQuery = new BooleanQuery.Builder().add(bc1).add(bc2).build();

2.5.5 范围搜索（RangeQuery）
查找满足一定范围的文档。在某范围内的搜索条件，实现从一个开始词条到一个结束词条的搜索。可以包含或不包含在内。
Query rangeQuery = IntPoint.newRangeQuery("reply",500,1000);

2.5.6 前缀搜索（PrefixQuery）
使用前缀来进行查找，首先定义一个词条Term，包含要抄着的字段名和前缀。然后构造一个 PrefixQuery对象，进行查找
Term term = new Term("title","学");
Query prefixQuery = new PrefixQuery(term);

2.5.7 多关键字搜索（PhraseQuery）
是一种 Phrase查询，查找几个不同的关键字，使用 PhraseQuery的add 方法添加关键字，通过 setSlop方法设定一个坡度，确定关键字之间是否允许或允许多少个无关词汇的存在。
PhraseQuery.Builder builder = new PhraseQuery.Builder();
builder.add(new Term("title","日本"),2);
builder.add(new Term("title","美国"),3);
PhraseQuery phraseQuery = builder.build();

2.5.8 模糊搜索（FuzzyQuery）
简单的识别两个相近的词语，流入 Trump拼成 Trmp或Tramp，则使用模糊搜索都可以搜索到正确结果。
简单的模糊查询，建立词项，并创建 FuzzyQuery对象
Term trem = new Term("title","Tramp");
FuzzyQuery fuzzyQuery = new FuzzyQuery(trem);

2.5.9 通配符搜索（WildcardQuery）
通配符的查询。在查询参数值种使用通配符，WildcardQuery会进行处理
WildcardQuery wildcardQuery = new WildcardQuery(new Term("title","学?"));

2.6 Lucene查询高亮
在一个标准的搜索引擎中高亮的命中结果，几乎是必不可少的一需求。

public static void test() throws IOException, ParseException, InvalidTokenOffsetsException {
        String field = "title";
        Path indexPath = Paths.get("indexdir");
        Directory dir = FSDirectory.open(indexPath);
        IndexReader reader = DirectoryReader.open(dir);
        IndexSearcher searcher = new IndexSearcher(reader);
        Analyzer analyzer = new IKAnalyzer6x();
        QueryParser parser = new QueryParser(field,analyzer);
        Query query = parser.parse("北大");
        System.out.println("Query:"+query);
        QueryScorer score = new QueryScorer(query,field);
        SimpleHTMLFormatter fors = new SimpleHTMLFormatter("<span style=\"color:red;\">","</span>");
        Highlighter highlighter = new Highlighter(fors,score);

        TopDocs tds = searcher.search(query,10);
        for(ScoreDoc sd : tds.scoreDocs){
            Document doc = searcher.doc(sd.doc);
            System.out.println("id:"+doc.get("id"));
            System.out.println("title:"+doc.get("title"));
            TokenStream tokenStream = TokenSources.getAnyTokenStream(searcher.getIndexReader(),sd.doc,field,analyzer);

            Fragmenter fragment = new SimpleSpanFragmenter(score);
            highlighter.setTextFragmenter(fragment);
            String str = highlighter.getBestFragment(tokenStream,doc.get(field));
            System.out.println("高亮片段："+str);
        }
        dir.close();
        reader.close();
    }

  2.7 Lucene新闻高频词提取
  2.7.1 提出问题
  2.7.2 需求分析
  索引过程的本质是一个词条化的生存倒排索引的过程。最后生成词项。然后统计词项在文档中的出现次数，进行降序排序。
  2.7.3 变成实现
  （参看代码）

  第三章 Lucene文件检索项目实践
  3.1 需求分析

  3.2 架构设计
  业务流程过程：文件存储系统中存放了不同类型的文件，后台功过程序题去了文件名和文档内容，使用Lucene对文件名和文档内容进行索引，前端对用户提供查询接口，用户提交关键字之后检索索引数据库，返回匹配文档至前端页面。一般会使用 开源工具 Tika 完成信息抽取，使用Lucene构建索引，使用JSP给用户提供查询接口，使用 Servlet完成搜索。

  3.3 文本内容抽取
  使用 Apache Tika 工具
  3.3.1 Tika简介
  用于文件类型检测和文件内容提取的库，目标群体主要为搜索引擎以及其他内容索引和分析工具，编程Tika通过提供一个通用的API来检测并提取多种文件格式的内容服务来达到这一目的。
  特点：统一解析器接口，内存占用低，快速处理，灵活元数据，解析器继承，MIME类型检测，语言检测。

  3.3.2 Tika下载 
  从 https://tika.apache.org/download.html 下载相应的 jar包。解压启动使用。java -jar tika-app-1.13.jar -g
  可以使用菜单进行操作使用，

  3.3.3 搭建工程
  下载 jar包，放入 lib目录下可放入测试文件，编写分析类 

  3.3.4 内容抽取
  在 分析类中抽取数据
  File pdfFile = new File(filepath);
        BodyContentHandler handler = new BodyContentHandler();  //创建内容处理器对象
        Metadata metadata = new Metadata();  //创建元数据对象
        FileInputStream inputStream = new FileInputStream(pdfFile);
        ParseContext parseContext = new ParseContext();  //创建内容解析器对象
        PDFParser parser = new PDFParser();    //创建PDF解析器对象
        parser.parse(inputStream,handler,metadata,parseContext);   //解析文档
        System.out.println("文件属性信息：");
        for(String name : metadata.names()){
            System.out.println(name+" : "+metadata.get(name));
        }

 过程：提取的文件的文本内容，需要实例化 BodycontentHandler，创建 Metadata对象用于获取文件属性。实例化一个 FileInputStream对象，将 pdf文件对象作为参数构造 FileInputStream，但是不支持随机访问，如果需要，可以使用 Tika提供的 TikaInputStream类。接下来创建一个解析上下文的 ParseContext对象，并实例化一个 PDF解析器对象，然后调用 PDF解析器对象的 parse方法，并传入所有需要的4个参数。解析完成后通过内容处理器对象的 toString方法输出文件内容，文件属性名保存在 元数据对象的 naes方法中，返回字符数组，便利属性名数组通过元数据对象的 get方法得到属性信息。
 不同的类型有不同的解析器
 OOXMLParser parser = new OOXMLParser();
 TXTParser parser = new TXTParser();
 HtmlParser parser = new HtmlParser();
 XMLParser parser = new XMLParser();
 ClassParser parser = new ClassParser();

 3.3.5 自动解析
 先判断文档类型，再根据文档类型实例化解析器接口。
 参见代码
 AutoDetectParser是 CompositeParser的子类，能够自动检测文件类型，并使用对应的方法把接收到的文档自动发送给最接近的解析器类。

 3.4 工程搭建
 略

 第四章 从 LucenedaoElasticsearch

 4.1 Elasticsearch概述
 4.1.1 诞生过程
 Lucene用于底层搜索，Elasticsearch用于企业应用
 基于 ES衍生了一系列开源软件，所搜引擎 Elasticsearch，日志采集与解析工具 Logstash，可视化平台工具 Kibana，简称 ESK Stack，是非常流行的集中式日志解决方案。
 ES公司后来推出了 Beats家族，在数据收集方面使用 Beats取代Logstash，解决性能问题。
 Beats家族产品：
 Filebeat 轻量级的日志采集器，可用于收集文件数据
 Metricbeat 搜集系统，进程和文件系统级别的 CPU和内存使用情况等数据。
 Packetbeat 收集网络流数据，可实时监控系统应用和服务，可以将延迟时间、错误、响应时间，SLA性能等信息发送到 Logstash或 ES
 Winlogbeat 搜集Windows事件日志数据
 Heartbeat 监控服务器运行情况

 4.1.2 流行度分析
 略

 4.1.3 架构解读
 ES整体是分为几个不同层次的，
 Gateway是ES用来存储索引的文件系统，支持 LocalFileSystem，SharedFileSystem，还可使用 Hadoop的 HDFS
 Gateway上层是一个分布式的 Lucene框架，ES的底层API是由 Lucene提供的，每个ES节点上都有一个 Lucene引擎的吃吃
 Lucene之上是 ES模块，包括索引模块，搜索模块映射解析模等。
 ES之上 是 Discovery，Scripting和第三方插件。其中 Discovery是ES的节点发现模块，节点组成集群需要进行消息通信，集群内部需要选举 Master节点，有 Discovery模块完成，Scripting用来支持 JavaScript，Python等多种语言，可以在查询语句中嵌入，使用 Script语句性能稍低。
 再上层是 ES的传输模块 和。JMX。支持 Thrift，Memcached，HTTP，默认使用 HTTP传输，JMX是 Java的管理框架，用来管理ES应用
 最上层是 ES提供给用户的接口，可以通过 RESTFul API和 ES集群进行交互

 4.1.4 优点
 分布式，全文检索，近实时搜索和分析，高可用，模式自由，RESTful API

 4.1.5 应用场景
 站内搜索，NoSQL数据库，日志分析

 4.1.6 核心概念
 1.集群：一个或多个安装了ES的服务器节点组织在一起就是集群。共同持有整个数据，并一起提供索引和搜索功能，一个集群由一个唯一的名字标示，具有相同的集群名称的节点才会组成一个集群。
 2.节点：一个节点是你集群中的一个服务器，存储数据，参与集群的索引和搜索功能。
 3.索引：一个索引就是一个拥有几分相似特征的文档的集合，索引的数据机构仍然是倒排索引。一个索引由一个名字来标示，进行操作时，都要使用这个名字。在集群中可以定义任意多的索引，索引做动词来将的时候表示索引数据和对数据进行索引操作。
 4.类型：索引中，定义一种或多种类型，类型是索引的一个逻辑上的分类或分区。具有一组共同字段的文档定义一个类型。
 5.文档：一个文档是一个可被索引的基础信息但愿。都是 JSON格式。
 6.分片：一个索引可以存储超出单个节点硬件限制的大量数据。ES将索引划分为多份，这些份就是分片。创建索引时可以指定分片的数量，每个分片本身是一个功能完整切独立的索引，这个索引可以放置到集群中的任何节点上。
 分片允许水平分割/扩展你的内容容量
 允许你在分片上进行分布式的，并行的操作，进而提高性能和吞吐量。
 7.副本：ES允许创建分片的一份活多份拷贝，这些拷贝叫做复制分片或直接叫副本。
 在分片/节点失败的情况下，保证高可用性。扩展搜索量/吞吐量，搜索是可以在所有副本上并行运行的。
 每个索引可以被分成多个分片，一个索引可以有一至多个副本，一旦有了副本，每个索引就有了主分片和副本分片之别。创建索引后，可以在任何时候情况下动态改变副本数量，分片数量不能欧冠修改。

 4.1.7 对比 RDMS
 ES可以看成一个数据库
 数据库（database）-》索引（index）
 表（table）-〉类型（type）
 行（row）-》文档（document）
 列（column）-〉字段（field）
 表结构（Schema）-》映射（Mapping）
 索引-〉全文索引
 SQL-》查询DSL
 SELECT * from。-〉GET http://xxxx
 UPDATE table SET -》 PUT http://xxxx
 DELETE -〉 DELETE http://xxxx

 4.1.8 文档结构
 文档是 ES的基本单位，ES中的文档搜是用 JSON来表示的，轻量级数据结构
 key/value 健值对结构，包括字段名称（在双引号中），后面一个冒号，然后是值。数组结构，也称为有序刘表。JSON值的类型可以是数字，字符串，布尔值，数组，对象，null。JSON对象在或括号中书写，对象可以包含多个名称/值对。

 4.2 安装 ES
 参考 OneOnte

 4.2.6 基本配置
 config目录是存放配置文件的地方，elasticsearch.yml是基本的配置文件，jvm.options 是虚拟机参数配置文件，log4j2.properties是日志配置文件。
cluster.name:my-application
node.name:node-1
node.master:true
node.data:true
index.number_of_shards:5
index.number_of_replicas:1
path.data: /path/to/data
path.logs: /path/to/logs
bootstrap.mlockall:true  //设置true来锁住内存。当 jvm开始 swapping时ES效率会降低，要保证不swap。
network.host：192.168.0.1
http.port
transport.tcp.port：9300  //TCP端口，JavaAPI使用的端口
transport.tcp.compress：true 是否压缩TCP传输时的数据
http.ax_content_length:100mb。设置内容的最大容量
http.cors.enabled:false 是否使用HTTP协议对外提供服务
discovery.zen.mminimum_master_nodes：1 保证集群中的节点可以知道其他N个有master资格的节点
discovery.zen.ping.timeout：3s 集群中自动发现其他节点的 ping链接超时时间
discovery.zen.ping.multicast.enabled：false 是否打开多播发现节点
discovery.zen.ping.unicast.hosts:["host1","host2:port","host3[portX-portY]"]  设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。
script.engine.groovy.inline.update：on 开启 groovy脚本支持
script.inline：true  开始所有脚本语言行内执行所有支持的操作。

4.3 中文分词器配置
4.3.1 IK分词器安装
索引是把文档写入ES的过程，搜索是匹配查询条件找出文档的过程。第一步是词条化，分词器把输入文本转化为一个个词条流,第二步是过滤，在这个阶段有若干个过滤器处理词条流中的词条。ES中内置了多种分词器可以使用。
Standard Analyzer：标准分词器
Simple Analyzer：简单分词器，基于非字母字符进行分词
Whitespace Analyzer：空格分词器
Stop Analyzer：同简单分词器，增加了停用词过滤功能
Keyword Analyzer：关键词分词器
Pattern Analyzer：正则分词器，单词会被小写，支持停用词
Language Analyzers：特定语言的分词器
Fingerprint Analyzer：指纹分析仪分词器

ES中使用的中文分词器使用最多的是 elasticsearch-analysis-ik，第三方插件， https://github.com/medcl/elasticsearch-analysis-ik 使用版本要与 ES一致

使用：下载后，放入 es的 /plugins/目录中，新建ik文件夹，将解压后的所有文件拷贝到 ik目录中。重启 ES服务

4.3.2 扩展本地词库
可以将扩展词文件放入  xxxx/plugins/ik/config/custom 目录下，新增 hotwords.dic 文件，在其中添加内容
然后在 xxxx/plugins/ik/config/IKAnalyzer.cfg.xml 文件中指定新增的词库位置。
<entry key="ext_dict">
<entry key="ext_stopwords">
<entry key="remote_ext_dict">
<entry key="remote_ext_stopwords">
扩展本地停用词的方法类似，配置完成后 重启 ES。

4.3.3 配置远程词库
在 ik插件的配置文件中指定新增的远程词库地址，然后重启ES，ik会自动重新加载词典。

4.4 Head插件使用指南
4.4.1 Head插件的安装 (参考 oneNode)
4.4.2 Head插件的使用（略）

4.5 REST命令
ES 应用各种任务的 RESTful API  REST的全称是 Representational State Transfer 翻译为：表述性状态转移
特点：
1.采用 客户-服务器 架构，通信只能由单方面发起
2.通信的会话状态由客户端维护
3.响应内容可以在同心链的某处被缓存，改善网络效率
4.通信链组件支架你通过统一的接口相互通信
5.采用分层系统（Layered System）通过限制组件行为，将架构分解为若干等级的层。
6.支持通过下载并执行一些代码，对客户端的功能进行扩展
优点：
高效的利用缓存提高响应速度
通信本身无状态性，可以让不同的服务器处理一系列请求中的不同请求，提高服务器的扩展性。
浏览器可以作为客户端
相对其他叠加在HTTP协议上的机制，REST的软件依赖性更小
不需要额外的资源发现机制
兼容性更好

符合 REST设计风格的 WebAPI 称为 RESTful API，REST是设计风格，不是标准。REST架构中的资源有 URI来指定的。对资源的操作，通过 GET，POST，PUT，DELETE 方法。资源的表现形式可以是 XML，HTML，JSON 或其他的格式

典型应用：
GET 获取，PUT 增加/更新，POST 创建/追加，DELETE 删除

4.5.1 CURL工具
CURL是利用 URL 语法在命令行方式下工作的开源文件传输工具，在Unix,Linux 中，支持多种通信协议。
Ubuntu安装： sudo apt-get install curl libcurl3 libcurl3-dev php5-curl.

4.5.2 Kibana Dev Tools

（参照 OneNode）

第五章 Elasticsearch 集群入门

5.1 索引管理
5.1.1 新建索引
使用 PUT xxxx 语法格式   ，如果索引名称含有大写字母，则会报错
如果添加的索引在 ES中已经存在，同样会报异常
ES默认给一个索引设置5个分片1个副本，分片数设置后不能修改，副本数随时可以修改，可以通过 settings参数设置初始化信息
PUT blog
{
	"settings":{
		"number_of_shards":3,
		"number_of_replicas":0
	}
}

5.1.2 更新副本
PUT blog/_settings
{
	"number_of_replicas":2
}

5.1.3 读写权限
对索引的读写操作进行限制
PUT blog/_settings
{
	"blocks.write":true,   #禁止对索引写操作
	"blocks.read_only":true,   #设置当前索引只允许读，不允许写或者更新
	"blocks.read":true  #禁止对索引读操作
}

5.1.4 查看索引
查看索引的配置信息，在 GET方法上加上 _setting 参数,查看索引的配置信息。
GET test/_settings
还可以同时查看多个索引的 setting信息，GET blog,twitter/_settings
或者查看急群众所有索引的setting信息，GET _all/_settings

5.1.5 删除索引
直接 Delete删除，索引中的文档也将不在。
DELETE blog    #如果删除的索引名不在，将会异常

5.1.6 索引的打开和关闭
一个关闭了的索引，不占系统资源 
POST blog/_close
POST blog/_open   #也可以同时打开或关闭多个索引
对不存在的索引进行 打开或关闭操作，则发生异常，可以使用 ignore_unavailable=true 参数操作只存在的索引
POST blog/_close?ignore_unavailable=true
POST _all/_close  或  POST test*/_close

5.1.7 复制索引
_reindex API 可以把文档从一个索引复制到另一个索引(复制文档信息)，目标索引不会复制源索引的配置信息。
POST _reindex
{
  "source":{"index":"test"},
  "dest": {"index":"test_news"}
}
在复制时，可以增加一些条件限制：
POST _reindex
{
  "source": {
    "index": "blog",
    "type": "article",
    "query": {
      "term": {"title":"git"}
      }
    },
    "dest": {
      "index":"blog_news"
    }
}

5.1.8 收缩索引
索引的分片初始化后无法在做修改，通过 shrink index AP 提供的缩小索引分片数机制，把一个索引变成一个更少分片的索引，收缩后的分片数必须是原始分片数的因子。收缩索引前，索引中的每个分片都要在同一个节点上。
在缩小前，索引必须被标识为制度，所有分片都会复制到一个相同的节点并且健康值为绿色。
PUT test/_settings
{
  "index.routing.allocation.requier._name":"shrink_node_name",
  "index.blocks.write":true
}

POST test/_shrink/test_new
{
  "settings":{
    "index.number_of_replicas":0,
    "index.number_of_shards":1,
    "index.codec":"best_compression"
  },
  "aliases":{
    "my_search_indices":{}
  }
}

5.1.9 索引别名
给索引或多个索引起的另一个名字
POST /_aliases
{
  "actions": [
    {"add": {"index": "test1","alias": "alias1"}}
  ]
}
移除别名：
POST /_aliases
{
  "actions": [
    {"remove": {"index": "test1","alias": "alias1"}}
  ]
}
并且可以同时操作多个索引。还可以有简写形式
POST /_aliases
{
	"actions":[
		{"add":{"indices":["test1","test2"],"alias":"alias1"}}
	]
}
也可以将 增加和删除混合使用
POST /_aliases
{
	"actions":[
		{"remove":{"index":"test1","alias":"alias1"}},
		{"add":{"index":"test1","alias":"alias1"}}
	]
}
说明：如果别名和索引是一对一的，则别名可以直接使用对应的文档。如果别名和索引是一对多的，则使用别名会发生错误。
ES 支持通过通配符同时给多个索引设置别名， 
{"add":{"index":"test*","alias":"alias1"}}

查看索引的别名：
GET /test/_alias

查看所有的别名信息：
GET /_alias

5.2 文档管理
ES中文档的操作和故事逆袭数据库操作非常相似。

5.2.1 新建文档
PUT blog/article/1
{
  "id":1,
  "title":"Git简介",
  "posttime":"2017-05-01",
  "content":"Git是一款免费、开源的分布式版本控制系统"
}
响应的信息中包含创建的文档所在的索引index，类型type，id，版本，分片，是否创建成功。版本号是递增的
如果不指定 id会自动生成，默认是字符串
采用默认方式生成的，必须使用 POST方式，PUT会出现异常

5.2.2 获取文档
使用 GET API 查看在ES中的文档，指定文档所在的索引，类型，id 即可返回一个 JSON文档
GET blog/article/1
在返回值中，found属性表明是否查询到文档，_source字段中是文档的内容

使用 HEAD命令查看一个文档是否存在
HEAD blog/article/1  #返回值巍峨 200 或 404

使用 MULTI GET API来根据索引名，类型名，id，一次获取多个文档，返回一个文档数组
GET _mget
{
  "docs":[
    {
      "_index":"blog",
      "_type":"article",
      "_id":"1"
    },
    {
      "_index":"twitter",
      "_type":"tweet",
      "_id":"2"
    }
  ]
}
如果没有找到文档的将返回 index_not_found_exception 异常
还可以在统一 index或同一个 type 下返回多个文档，那样就可以使用简写形式了。

5.2.3 更新文档
在ES中，需要先找到这个文档，删除旧文档内容执行更新，更新完后在索引最新的文档。
首先，索引一个文档
PUT test/type1/1
{
  "counter":1,
  "tags":["green"]
}

然后对其进行更新操作：
POST test/type1/1/_update
{
  "script": {
    "inline": "ctx._source.counter+=params.count",
    "lang":"painless",
    "params": {
      "count":4
    }
  }
}
其中：inline是执行脚本，ctx是脚本语言中的一个执行对象，painless是ES内置的一种脚本语言，params是参数集合。
上面的功能表述为：使用 painless脚本更新文档，通过ctx获取 _source 在修改 counter字段，counter字段等于原值加上count参数的值。
ctx对象除了可以访问 _socure之外，还可以访问 _index,_type,_id,_version,_routing,_parent 等字段

tags字段的取值为数组类型，可以对其操作，
POST test/type1/1/_update
{
  "script": {
    "inline": "ctx._source.tags.add(params.tag)",
    "lang": "painless",
    "params":{
      "tag":"blue"
    }
  }
}

给文档增加字段：
POST test/type1/1/_update
{
  "script": "ctx._source.new_field=\"value_of_new_field\""
}

移除字段：
POST test/type1/1/_update
{
  "script": "ctx._source.remove=(\"new_field\")"
}

删除文档中，条件是数组类型数据项：
POST test/type1/1/_update
{
  "script":{
    "inline":"if (ctx._source.tags.contains(params.tag)) {ctx.op=\"delete\" } else {ctx.op=\"none\"}",
    "lang":"painless",
    "params":{"tag":"red"}
  }
}

还有一个 upsert操作，如果文档存在则执行script脚本，否则创建文档。
POST test/type1/1/_update
{
  "script": {
    "inline": "ctx._source.counter+=params.count",
    "lang": "painless",
    "params": {"count":4}
  },
  "upsert": {
    "counter":1
  }
}

5.2.4 查询更新
使用 Update By Query API，支持条件查询更新文档
POST blog/_update_by_query
{
  "script":{
    "inline":"ctx._source.category=params.category",
    "lang":"painless",
    "params":{"category":"git"}
  },
  "query":{
    "term":{"title":"git"}
  }
}

5.2.5 删除文档
允许指定 id从索引库中删除一个文档。命令：
DELETE blog/article/1  #命令格式

如果在索引文档时使用了路由，则删除时也可以增加路由参数。
DELETE blog/article/1?routing=user123

5.2.6 查询删除
根据查询条件进行删除操作
POST blog/_delete_by_query
{
  "query":{
    "term":{
      "title":"hibernate"
    }
  }
}

还可以指定类型，然后批量删除
POST blog/csdn/_delete_by_query
{
  "query":{
    "match_all":{}
  }
}

5.2.7 批量操作
通过 Bulk API 执行批量操作，索引，删除，更新等。允许使用单一请求来实现多个文档的 create，index，update，delete
（略）

5.2.8 版本控制
ES在文档更新过程：1.读取源文档，对其进行更新操作。2.更新操作完成后，再重新索引整个文档，最后保存在ES中的时最后一次更新后的文档。但是多线程操作时，会发生冲突。
确保数据在并发更新时，有两种办法：
1.悲观锁控制：屏蔽一切有可能违反数据完整性的操作。
2.乐观锁控制：只有在提交数据操作时，检查是否违反数据完整性，ES使用乐观锁机制，省去开销，提高吞吐量。
ES是一个分布式系统，文档的操作要复制到其他接待你，ES也是异步并发的，ES会处理确保旧版本文档不会被覆盖较新版本的文档。
版本号：文档每次修改，版本号都会自增一次。version字段确保所有更新都有序进行。旧版本的数据更新会被忽略。
ES文档版本控制有内部版本控制和外部版本控制。内部版本控制要求每次操作请求，只有当版本号相符时才能操作成功。外部版本控制要求外部文档比内部文档版本高时才能更新成功。
PUT website   #创建新的索引

PUT /website/blog/1。   #添加一个文档
{
  "title":"My first blog entry",
  "text":"Just trying this out ..."
}

GET /website/blog/1  #查看文档返回的版本号

POST website/blog/1/_update   #更新一个文档
{
  "script": "ctx._source.title=\"Update My first blog\""
}

GET /website/blog/1?version=1   #指定版本号查看文档，如果版本与当前不一致会返回异常

#更新文档时，可以指定外部文档的版本号，如果外部版本号不高于当前文档版本，同样会发生异常。
PUT website/blog/1?version=4&version_type=external
{
  "title":"My blog entry",
  "text":"JStarting to get the hang of this ..."
}

5.2.9 路由机制
路由机制时通过哈希算法，将具有相同哈希值的文档放置到同一个主分片中
计算语法：shard= hash(routing) % number_of_primary_shards
ES默认将文档ID作为 routing值，这个值范围在 0～number_of_primary_shards-1 来确定特定文档所在的分片。从而数据在所有分片上的一个平均分布。
自定义 routing值，能够带来很多使用上的方便和性能上的提升。
按照 ES默认的路由规则，数据文档ID将平均分布在所有的分片上，但是导致ES不能确定文档的文职，必须将请求光报道所有的分片上去执行。而且分片数不能够改变，改变后，之前路由值会变成非法的，等于丢失了。使用自定义的路由更具目的性，告诉ES数据在那个分片上。
ES的 index，get，mget，delete，update 等 API文档都可以接受一个 routing参数。例如：
PUT /website/blog/1?routing=user123
{
  "title":"My first blog entry",
  "text":"Just trying this out ..."
}
在使用时，可以通过 routing值进行过滤，这样可以避免ES向所有的分片都发送请求查询
GET /website/blog/1?routing=user123

切记：在使用时，如果添加时使用了路由，那么查询时候也要使用路由参数，否则查询不到。而且，带与不带路由，两个相同的数据并不冲突。

可以为文档指定多个路由值，路由值之间使用逗号隔开
问题：比如造成一个路由下的文档数量非常多，导致分片较大，出现数据偏移情况。特别是多个这样的用户处于同一分片的时候会更加明显。

5.3 映射详解
映射就是 Mapping，定义一个文档以及其包含的字段如何被存储和索引，可在映射中事先定义字段的数据类型、分词器等属性。
ES在创建索引时，同样可以设置字段的属性，作用是使所用的配置更加灵活和完善，可在Mapping中设置字段的类型，字段的权重等信息。

5.3.1 映射分类
映射分为动态映射和静态映射，ES会根据字段类型自动识别（动态映射），静态映射则是写入数据之前对字段的属性进行手工设置。

5.3.2 动态映射
推测规则： 
null-》没有字段被添加，
true or false -〉boolean类型，
浮点类型数字-》float类型，
数字-〉long类型，
JSON对象-》object类型，
数组-〉由数组中的第一个非空值决定，
string-》有可能是 date类型，double，long，text，keyword类型

如果将ES当作主要的数据存储使用，并且希望出现未知字段时抛出异常来提醒，那么要关闭Mapping的动态映射。
在 Mapping中，通过 dynamic来设置是否自动新增字段。true（默认的）/false（忽略新字段）/strict（严格模式，抛出异常）
PUT books
{
  "mappings": {
    "it":{
      "dynamic":"strict",
      "properties":{
        "title":{
          "type":"text"
        },
        "publish_date":{
          "type":"date"
        }
      }
    }
  }
}
如果使用新字段，则抛出 strict_dynamic_mapping_exception 异常
PUT books/it/3
{
  "title":"master Elasticsearch",
  "publish_date":"2017-06-01",
  "author":"Tom"
}

5.3.3 日期检测
ES在碰到一个新的字符串类型时，会检查这个字符串是否包含一个可识别的日期，如果看起来像日期，则被识别为一个date类型的字段，否则将会当作string字段进行添加。
可以根据类型（type）将，date_detection设置为 false来关闭日期检测。
PUT /my_index
{
  "mappings": {
    "my_type":{
      "date_detection":"false"
    }
  }
}

5.3.4 静态映射
创建索引时，手工指定索引映射，通过静态映射可以添加更详细，更精准的配置信息。
PUT my_index
{
  "mappings":{
    "user":{
      "_all":{"enabled":false},
      "properties":{
        "title":{"type":"text"},
        "name":{"type":"text"},
        "age":{"type":"integer"}
      }
    },
    "blogpost":{
      "_all":{"enabled":false},
      "properties":{
        "title":{"type":"text"},
        "body":{"type":"text"},
        "user_id":{"type":"keyword"},
        "created":{
          "type":"date",
          "format":"strict_date_optional_time||epoch_millis"
        }
      }
    }
  }
}
说明：这段代码会引发异常。在Elasticsearch 6.0.0或更高版本中创建的索引可能只包含单个mapping type。在具有多种映射类型的5.x中创建的索引将继续像以前一样在Elasticsearch 6.x中运行。映射类型将在Elasticsearch 7.0.0中完全删除。

5.3.5 字段类型
ES字段类型主要有 核心类型，复合类型，地理类型，特殊类型
核心类型：字符串类型（string，text，keyword），数字类型（long，integer，short，byte，double，float，half_float，scaled_float），日期类型（date），布尔类型（boolean），二进制类型（binary），范围类型（range）

复合类型：数组类型（array），对象类型（object），嵌套类型（nested）

地理类型：地理坐标（geo_point），地理图形（geo_shape）

特殊类型：IP类型（ip），范围类型（completion），令牌计数类型（token_count），附件类型（attachment），抽取类型（percolator）

1.string，自ES 5.x 之后，字段类型不再支持string，使用 text，keyword取代
2.text：如果一个字段是被全文搜索的，应该使用这个类型，字段内容会被分析，生成倒排索引，会被分词器分成一个一个慈祥，该类型不用于排序，何少用于聚合
3.keyword：用于索引结构化的字段，用于过滤，排序，聚合。只能通过精确值搜索到
4.对于数字类型的字段，满足需求的清咖滚下，尽可能选择范围小的数据类型，处理浮点时，优先考虑使用 scaled_float 类型，采用这种缩放类型的浮点类型，更加节省空间存储
5.ES中没有日期类型，可以使用几种形式：1.格式化日期的字符串，2.代表 milliseconds-since-the-epoch 的长整数类型（1970-1-1 UTC 时间），3.代表 seconds-since-the-epoch 的整数类型，ES内部会把日期转换为 UTC，并存储为表示 milliseconds-since-the-epoch 的长整数类型。默认格式为 "strict_date_optional_time || epoch_millis"
PUT my_index/blogpost/1
{
  "created":"2005-01-01"
}
PUT my_index/blogpost/2
{
  "created":"1420070400001"
}
7.binary：接受base64编码的字符串，默认不存储，也不可搜索
PUT my_index/my_type/1
{
  "name":"Some binary blob",
  "blob":"U29tZSBiaW5hcnkgYmxvYg=="
}
8.array: 没有专用的数组类型，默认情况下任何字段都可以包含一个或多个值，但一个数组中的值必须是同一种类型
字符数组：["one","two"]
整型数组：[1,3]
嵌套数组：[1,[2,3]] 等价于 [1,2,3]
对象数组：[{"name":"Mary","age":12},{"name":"John","age":10}]
动态添加数据时，数组的第一个值的类型决定整个数组的类型。混合数组类型是不支持的。数组可以包含 null值
PUT my_index2/my_type/1
{
  "message":"some arrays in this document...",
  "tags":["elasticsearch","wow"],
  "lists":[
      {
        "name":"prog_list",
        "description":"programming list"
      },
      {
        "name":"cool_list",
        "description":"cool stuff list"
      }
    ]
}
查询时使用：
GET my_index2/_search
{
  "query": {
    "match": {
      "lists.name": "cool_list"
    }
  }
}

9.object: JSON本质上有层级关系，文档包含内部对象，内部对象本身还包含内部对象。
PUT my_index2/my_type/2
{
  "region":"US",
  "manager":{
    "age":30,
    "name":{
      "first":"John",
      "last":"Smith"
    }
  }
}
写入到ES中后，文档会被索引成简单的扁平 key-value对，格式如下：
{
	"region":"US",
	"manager.age":30,
	"manager.name.first":"John",
	"manager.name.last":"Smith"
}

10.nested
是 object类型中的一个特例，让对象数组独立索引和查询。ES将对象层次扁平化，转化成字段名字和值构成的简单列表。
PUT my_index2/my_type/1
{
  "group":"fans",
  "usr":[
      {
        "first":"John",
        "last":"Smith"
      },
      {
        "first":"Alice",
        "last":"White"
      }
    ]
}
扁平化转换：
{
  "group":"fans",
  "user.first":["alice","john"],
  "user.last":["smith","white"]
}
扁平化后，user.first 和 user.last 会变为多值字段，alice和 white的关系关系丢失了。
在搜索的时候，可以按照层级的方式进行使用
如果需要使用索引对象数组，并避免上述问题的产生，因使用 nested对象类型而不是object类型。
nested对象类型可以保持数组中每个对象的独立性，讲数组中的每个对象作为独立要隐藏文档来索引，每个嵌套对象都可以独立被搜索。
PUT /my_index
{
	"mappings":{
    "my_type":{
      "properties":{
        "user":{"type":"nested"}
      }
    }
}

11.geo_point
存储地理位置信息的经纬度。可以 查找一定范围内的地址位置，通过地址位置或相对中心店的距离来聚合文档。吧距离因素整合到文档的评分中。通过距离对文档排序。
PUT my_index
{
  "mappings": {
    "my_type":{
      "properties":{
        "location":{"type":"geo_point"}
      }
    }
  }
}
geo_pint 支持四中类型的地址位置数据：
1.经纬度坐标键值对
PUT my_index/my_type/1
{
  "text":"Geo-point as an object",
  "location":{
    "lat":41.12,
    "lon":-71.34
  }
}
2.字符串格式的地理坐标参数
PUT my_index/my_type/2
{
  "text":"Geo-point as an object",
  "location":"41.12,-71.34"
  }
}
3.地理坐标的哈希值
PUT my_index/my_type/3
{
  "text":"Geo-point as an object",
  "location":"derm3btev3e86"
  }
}
4.数组形式的地理坐标
PUT my_index/my_type/4
{
  "text":"Geo-point as an object",
  "location":[-71.34,41.12]
  }
}

12.geo_shape
存储一块区域，GeoJSON是一种对各种地理数据结构进行编码的格式，包含一个几何对象和其他属性。
GeoJSON类型-》Elasticsearch类型-》说明
Point-》point-》一个单独的精度维度坐标点
LineString-》linestring-》任意的线条，由两到多个点组成
Polygon-》polygon-》由N+1点组成的封闭N边型
MultiPoint-》multipoint-》一组不连续但有可能相关连得点
MultiLineString-》multilinestring-》多条不关联的线
MultiPolygon-》multipolygon-》多个不关联的多边形
GeometryCollection-》geometrycollection-》几何对象的集合
N/A-》envelope-》由左上角坐标或右下角坐标确定的封闭矩形
N/A-》circle-》由圆心和半径确定的圆，默认单位为米

创建图形使用的 索引：
PUT geoshape
{
  "mappings": {
    "city":{
      "properties":{
        "location":{
          "type":"geo_shape"
        }
      }
    }
  }
}
根据不同类型，写入不同数据：
POST geoshape/city/1
{
  "location":{
    "type":"point",
    "coordinates":[
      -77.03653,
      38.897676
    ]
  }
}
POST geoshape/city/2
{
  "location":{
    "type":"linestring",
    "coordinates":[[-77.03653,38.897676],[-77.009051,38.889939]]
  }
}
POST geoshape/city/3
{
  "location":{
    "type":"polygon",
    "coordinates":[[100.0,0.0],[101.0,0.0],[101.0,1.0],[100.0,1.0],[100.0,0.0]]
  }
}

13.ip
存储 IPv4，或 IPv6的地址
PUT geoshape
{
  "mappings": {
    "city":{
      "properties":{
        "ip_addr":{
          "type":"ip"
        }
      }
    }
  }
}
使用：
PUT my_index/my_type/1
{
  "id_addr":"192.168.1.1"
}
GET my_index/_search
{
  "query": {
    "term": {
      "ip_addr": "192.168.0.0/16"
    }
  }
}

14.range
范围类型，比如时间范围选择，年龄范围选择等
不同的类型范围：integer_range，float_range，long_range，double_range，date_range
PUT range_index
{
  "mappings": {
    "my_type":{
      "properties":{
        "expected_attendees":{
          "type":"integer_range"
        },
        "time_frame":{
          "type":"date_range",
          "format":"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
        }
      }
    }
  }
}
放入数据时，使用：
PUT range_index/my_type/1
{
  "expected_attendees":{
    "gte":10,
    "lte":20
  },
  "time_frame":{
    "gte":"2015-10-31 12:00:00",
    "lte":"2015-11-01"
  }
}

15.token_count
用于统计字符串分词后的词项个数，本质上是一个整数型字段。
PUT my_index
{
  "mappings": {
    "my_type":{
      "properties":{
        "name":{
          "type":"text",
          "fields":{
            "length":{
              "type":"token_count",
              "analyzer":"standard"
            }
          }
        }
      }
    }
  }
}
这个例子中，映射指定name为text类型，增加 name.length 字段用来统计分词后词项的长度。类型为 token_count，使用标准的分词器。
PUT token_count_index/my_type/1
{
  "name":"John Smith"
}

PUT token_count_index/my_type/2
{
  "name":"Rachel Alice Williams"
}
然后进行查询：
GET token_count_index/_search
{
  "query": {
    "term": {
      "name.length": {
        "value": "3"
      }
    }
  }
}

5.3.6 元字段
描述文档的字段
文档属性的元字段（_index,_uid,_type,_id）
源文档的元字段（_source,_size）
索引的元字段（_all,_field_names）
路由的元字段（_parent,_routing）
自定义元字段（_meta）
1._index
支持对索引名进行 term查询，terms查询，聚合分析，使用脚本和排序。_index 是一个虚拟字段
PUT index_1/my_type/1
{
  "text":"Document in index1"
}

PUT index_2/my_type/2?refresh=true
{
  "text":"Document in index2"
}
使用时：
GET index_1,index_2/_search
{
  "query": {
    "terms":{"_index":["index_1","index_2"]}
  },
  "aggs":{
    "indices":{
      "terms": {
        "field": "_index",
        "size": 10
      }
    }
  },
  "sort":[
      {"_index":{"order":"asc"}}
    ]
}

2._type
每条被索引的文档都有一个 _type和 _id字段，可根据_type 进行查询、聚合、脚本和排序。
GET my_index/_search
{
  "query": {
    "term": {
      "_type": {
        "value": ["type_1","type_2"]
      }
    }
  },
  "aggs": {
    "types": {
      "terms": {
        "field": "_type",
        "size": 10
      }
    }
  },
  "sort": [{"_type": {"order": "desc"}}]
}

3._id
每条被索引的文档都有一个 _type和 _id字段，可根据_id 进行 term查询，terms查询，match查询，query_string查询，simple_query_string 查询。不能用于 聚合，脚本和排序。
GET my_index/_search
{
  :"query": {
    "terms":{"_id":["1","2"]}
  }
}

4. _uid
这个是 _type 与 _id 的组合，取值为 {type}#{id},可用于查询，聚合，脚本和排序。
GET my_index/_search
{
  "query":{
    "terms":{
      "_uid":["my_type#1","my_type#2"]
    }
  },
  "aggs":{
    "UIDs":{
      "terms":{
        "field":"_uid",
        "size":10
      }
    }
  },
  "sort":[
    {
      "_uid":{ "order":"desc"}
    }
  ]
}

5._source
其中 _source 存储文档的原始值，默认该字段是开启的，可以砸映射中通过 enabled参数关闭
PUT tweets
{
  "mappings": {
    "tweet":{
      "_source":{
        "enabled":false
      }
    }
  }
}

6._size
用于描述文档本身的字节大小，默认不支持的。有需要的话，需要安装 mapper-size 插件

7._all
该字段是把其他字段拼接在一起的超级字段，所有字段内容用空格分开，会被解析和索引，但不存储。
当需要返回某个关键字的文档，但不明确第搜索某个字段的时候，可使用 _all字段搜索
PUT my_index/blog/1
{
  "title":"Master Java",
  "content":"learn java",
  "author":"Tom"
}

GET my_index/_search
{
  "query": {
    "match": {
      "_all": "Java"
    }
  }
}

8._field_names (略)
用来存储文档中的所有非空字段的名字，常用语 exists查询

9._parent 
用于指定同一个索引中文档的父子关系
PUT my_index
{
  "mappings": {
    "my_parent":{},
    "my_child":{
      "_parent":{
        "type":"my_parent"
      }
    }
  }
}
PUT my_index/my_parent/1
{
  "text":"This is a parent document"
}
PUT my_index/my_child/2?parent=1
{
  "text":"This is a child document"
}
PUT my_index/my_child/3?parent=1&refresh=true
{
  "text":"This is another child document"
}
GET my_index/my_parent/_search
{
  "query": {
    "has_child": {
      "type": "my_child",
      "query": {
        "match": {
          "text": "child document"
        }
      }
    }
  }
}