2.5.3 词项搜索（TermQuery）
在搜索引擎中，最基本的搜索就是在索引中搜索某一词条，词条是最基本的搜索单位。 
词条就是一个 key/value对。key是字段名，value则表示字段中所包含的某个关键字。
Term term = new Term("title","美国");
Query termQuery = new TermQuery(term);

2.5.4 布尔搜索（BooleanQuery）
是一个组合的Query，把各种 Query添加进去，表明之间的逻辑关系。提供了专门的API，往其添加子句，BooleanClause对象可以指定查询的包含关系，并作为参数通过 BooleanQuery.Builder()构造布尔查询。
Query query1 = new TermQuery(new Term("title","美国"))；
Query query2 = new TermQuery(new Term("content","日本"))
BooleanClause bc1 = new BooleanClause(query1, BooleanClause.Occur.MUST);
BooleanClause bc2 = new BooleanClause(query2, BooleanClause.Occur.MUST_NOT);
BooleanQuery boolQuery = new BooleanQuery.Builder().add(bc1).add(bc2).build();

2.5.5 范围搜索（RangeQuery）
查找满足一定范围的文档。在某范围内的搜索条件，实现从一个开始词条到一个结束词条的搜索。可以包含或不包含在内。
Query rangeQuery = IntPoint.newRangeQuery("reply",500,1000);

2.5.6 前缀搜索（PrefixQuery）
使用前缀来进行查找，首先定义一个词条Term，包含要抄着的字段名和前缀。然后构造一个 PrefixQuery对象，进行查找
Term term = new Term("title","学");
Query prefixQuery = new PrefixQuery(term);

2.5.7 多关键字搜索（PhraseQuery）
是一种 Phrase查询，查找几个不同的关键字，使用 PhraseQuery的add 方法添加关键字，通过 setSlop方法设定一个坡度，确定关键字之间是否允许或允许多少个无关词汇的存在。
PhraseQuery.Builder builder = new PhraseQuery.Builder();
builder.add(new Term("title","日本"),2);
builder.add(new Term("title","美国"),3);
PhraseQuery phraseQuery = builder.build();

2.5.8 模糊搜索（FuzzyQuery）
简单的识别两个相近的词语，流入 Trump拼成 Trmp或Tramp，则使用模糊搜索都可以搜索到正确结果。
简单的模糊查询，建立词项，并创建 FuzzyQuery对象
Term trem = new Term("title","Tramp");
FuzzyQuery fuzzyQuery = new FuzzyQuery(trem);

2.5.9 通配符搜索（WildcardQuery）
通配符的查询。在查询参数值种使用通配符，WildcardQuery会进行处理
WildcardQuery wildcardQuery = new WildcardQuery(new Term("title","学?"));

2.6 Lucene查询高亮
在一个标准的搜索引擎中高亮的命中结果，几乎是必不可少的一需求。

public static void test() throws IOException, ParseException, InvalidTokenOffsetsException {
        String field = "title";
        Path indexPath = Paths.get("indexdir");
        Directory dir = FSDirectory.open(indexPath);
        IndexReader reader = DirectoryReader.open(dir);
        IndexSearcher searcher = new IndexSearcher(reader);
        Analyzer analyzer = new IKAnalyzer6x();
        QueryParser parser = new QueryParser(field,analyzer);
        Query query = parser.parse("北大");
        System.out.println("Query:"+query);
        QueryScorer score = new QueryScorer(query,field);
        SimpleHTMLFormatter fors = new SimpleHTMLFormatter("<span style=\"color:red;\">","</span>");
        Highlighter highlighter = new Highlighter(fors,score);

        TopDocs tds = searcher.search(query,10);
        for(ScoreDoc sd : tds.scoreDocs){
            Document doc = searcher.doc(sd.doc);
            System.out.println("id:"+doc.get("id"));
            System.out.println("title:"+doc.get("title"));
            TokenStream tokenStream = TokenSources.getAnyTokenStream(searcher.getIndexReader(),sd.doc,field,analyzer);

            Fragmenter fragment = new SimpleSpanFragmenter(score);
            highlighter.setTextFragmenter(fragment);
            String str = highlighter.getBestFragment(tokenStream,doc.get(field));
            System.out.println("高亮片段："+str);
        }
        dir.close();
        reader.close();
    }

  2.7 Lucene新闻高频词提取
  2.7.1 提出问题
  2.7.2 需求分析
  索引过程的本质是一个词条化的生存倒排索引的过程。最后生成词项。然后统计词项在文档中的出现次数，进行降序排序。
  2.7.3 变成实现
  （参看代码）

  第三章 Lucene文件检索项目实践
  3.1 需求分析

  3.2 架构设计
  业务流程过程：文件存储系统中存放了不同类型的文件，后台功过程序题去了文件名和文档内容，使用Lucene对文件名和文档内容进行索引，前端对用户提供查询接口，用户提交关键字之后检索索引数据库，返回匹配文档至前端页面。一般会使用 开源工具 Tika 完成信息抽取，使用Lucene构建索引，使用JSP给用户提供查询接口，使用 Servlet完成搜索。

  3.3 文本内容抽取
  使用 Apache Tika 工具
  3.3.1 Tika简介
  用于文件类型检测和文件内容提取的库，目标群体主要为搜索引擎以及其他内容索引和分析工具，编程Tika通过提供一个通用的API来检测并提取多种文件格式的内容服务来达到这一目的。
  特点：统一解析器接口，内存占用低，快速处理，灵活元数据，解析器继承，MIME类型检测，语言检测。

  3.3.2 Tika下载 
  从 https://tika.apache.org/download.html 下载相应的 jar包。解压启动使用。java -jar tika-app-1.13.jar -g
  可以使用菜单进行操作使用，

  3.3.3 搭建工程
  下载 jar包，放入 lib目录下可放入测试文件，编写分析类 

  3.3.4 内容抽取
  在 分析类中抽取数据
  File pdfFile = new File(filepath);
        BodyContentHandler handler = new BodyContentHandler();  //创建内容处理器对象
        Metadata metadata = new Metadata();  //创建元数据对象
        FileInputStream inputStream = new FileInputStream(pdfFile);
        ParseContext parseContext = new ParseContext();  //创建内容解析器对象
        PDFParser parser = new PDFParser();    //创建PDF解析器对象
        parser.parse(inputStream,handler,metadata,parseContext);   //解析文档
        System.out.println("文件属性信息：");
        for(String name : metadata.names()){
            System.out.println(name+" : "+metadata.get(name));
        }

 过程：提取的文件的文本内容，需要实例化 BodycontentHandler，创建 Metadata对象用于获取文件属性。实例化一个 FileInputStream对象，将 pdf文件对象作为参数构造 FileInputStream，但是不支持随机访问，如果需要，可以使用 Tika提供的 TikaInputStream类。接下来创建一个解析上下文的 ParseContext对象，并实例化一个 PDF解析器对象，然后调用 PDF解析器对象的 parse方法，并传入所有需要的4个参数。解析完成后通过内容处理器对象的 toString方法输出文件内容，文件属性名保存在 元数据对象的 naes方法中，返回字符数组，便利属性名数组通过元数据对象的 get方法得到属性信息。
 不同的类型有不同的解析器
 OOXMLParser parser = new OOXMLParser();
 TXTParser parser = new TXTParser();
 HtmlParser parser = new HtmlParser();
 XMLParser parser = new XMLParser();
 ClassParser parser = new ClassParser();

 3.3.5 自动解析
 先判断文档类型，再根据文档类型实例化解析器接口。
 参见代码
 AutoDetectParser是 CompositeParser的子类，能够自动检测文件类型，并使用对应的方法把接收到的文档自动发送给最接近的解析器类。

 3.4 工程搭建
 略

 第四章 从 LucenedaoElasticsearch

 4.1 Elasticsearch概述
 4.1.1 诞生过程
 Lucene用于底层搜索，Elasticsearch用于企业应用
 基于 ES衍生了一系列开源软件，所搜引擎 Elasticsearch，日志采集与解析工具 Logstash，可视化平台工具 Kibana，简称 ESK Stack，是非常流行的集中式日志解决方案。
 ES公司后来推出了 Beats家族，在数据收集方面使用 Beats取代Logstash，解决性能问题。
 Beats家族产品：
 Filebeat 轻量级的日志采集器，可用于收集文件数据
 Metricbeat 搜集系统，进程和文件系统级别的 CPU和内存使用情况等数据。
 Packetbeat 收集网络流数据，可实时监控系统应用和服务，可以将延迟时间、错误、响应时间，SLA性能等信息发送到 Logstash或 ES
 Winlogbeat 搜集Windows事件日志数据
 Heartbeat 监控服务器运行情况

 4.1.2 流行度分析
 略

 4.1.3 架构解读
 ES整体是分为几个不同层次的，
 Gateway是ES用来存储索引的文件系统，支持 LocalFileSystem，SharedFileSystem，还可使用 Hadoop的 HDFS
 Gateway上层是一个分布式的 Lucene框架，ES的底层API是由 Lucene提供的，每个ES节点上都有一个 Lucene引擎的吃吃
 Lucene之上是 ES模块，包括索引模块，搜索模块映射解析模等。
 ES之上 是 Discovery，Scripting和第三方插件。其中 Discovery是ES的节点发现模块，节点组成集群需要进行消息通信，集群内部需要选举 Master节点，有 Discovery模块完成，Scripting用来支持 JavaScript，Python等多种语言，可以在查询语句中嵌入，使用 Script语句性能稍低。
 再上层是 ES的传输模块 和。JMX。支持 Thrift，Memcached，HTTP，默认使用 HTTP传输，JMX是 Java的管理框架，用来管理ES应用
 最上层是 ES提供给用户的接口，可以通过 RESTFul API和 ES集群进行交互

 4.1.4 优点
 分布式，全文检索，近实时搜索和分析，高可用，模式自由，RESTful API

 4.1.5 应用场景
 站内搜索，NoSQL数据库，日志分析

 4.1.6 核心概念
 1.集群：一个或多个安装了ES的服务器节点组织在一起就是集群。共同持有整个数据，并一起提供索引和搜索功能，一个集群由一个唯一的名字标示，具有相同的集群名称的节点才会组成一个集群。
 2.节点：一个节点是你集群中的一个服务器，存储数据，参与集群的索引和搜索功能。
 3.索引：一个索引就是一个拥有几分相似特征的文档的集合，索引的数据机构仍然是倒排索引。一个索引由一个名字来标示，进行操作时，都要使用这个名字。在集群中可以定义任意多的索引，索引做动词来将的时候表示索引数据和对数据进行索引操作。
 4.类型：索引中，定义一种或多种类型，类型是索引的一个逻辑上的分类或分区。具有一组共同字段的文档定义一个类型。
 5.文档：一个文档是一个可被索引的基础信息但愿。都是 JSON格式。
 6.分片：一个索引可以存储超出单个节点硬件限制的大量数据。ES将索引划分为多份，这些份就是分片。创建索引时可以指定分片的数量，每个分片本身是一个功能完整切独立的索引，这个索引可以放置到集群中的任何节点上。
 分片允许水平分割/扩展你的内容容量
 允许你在分片上进行分布式的，并行的操作，进而提高性能和吞吐量。
 7.副本：ES允许创建分片的一份活多份拷贝，这些拷贝叫做复制分片或直接叫副本。
 在分片/节点失败的情况下，保证高可用性。扩展搜索量/吞吐量，搜索是可以在所有副本上并行运行的。
 每个索引可以被分成多个分片，一个索引可以有一至多个副本，一旦有了副本，每个索引就有了主分片和副本分片之别。创建索引后，可以在任何时候情况下动态改变副本数量，分片数量不能欧冠修改。

 4.1.7 对比 RDMS
 ES可以看成一个数据库
 数据库（database）-》索引（index）
 表（table）-〉类型（type）
 行（row）-》文档（document）
 列（column）-〉字段（field）
 表结构（Schema）-》映射（Mapping）
 索引-〉全文索引
 SQL-》查询DSL
 SELECT * from。-〉GET http://xxxx
 UPDATE table SET -》 PUT http://xxxx
 DELETE -〉 DELETE http://xxxx

 4.1.8 文档结构
 文档是 ES的基本单位，ES中的文档搜是用 JSON来表示的，轻量级数据结构
 key/value 健值对结构，包括字段名称（在双引号中），后面一个冒号，然后是值。数组结构，也称为有序刘表。JSON值的类型可以是数字，字符串，布尔值，数组，对象，null。JSON对象在或括号中书写，对象可以包含多个名称/值对。

 4.2 安装 ES
 参考 OneOnte

 4.2.6 基本配置
 config目录是存放配置文件的地方，elasticsearch.yml是基本的配置文件，jvm.options 是虚拟机参数配置文件，log4j2.properties是日志配置文件。
cluster.name:my-application
node.name:node-1
node.master:true
node.data:true
index.number_of_shards:5
index.number_of_replicas:1
path.data: /path/to/data
path.logs: /path/to/logs
bootstrap.mlockall:true  //设置true来锁住内存。当 jvm开始 swapping时ES效率会降低，要保证不swap。
network.host：192.168.0.1
http.port
transport.tcp.port：9300  //TCP端口，JavaAPI使用的端口
transport.tcp.compress：true 是否压缩TCP传输时的数据
http.ax_content_length:100mb。设置内容的最大容量
http.cors.enabled:false 是否使用HTTP协议对外提供服务
discovery.zen.mminimum_master_nodes：1 保证集群中的节点可以知道其他N个有master资格的节点
discovery.zen.ping.timeout：3s 集群中自动发现其他节点的 ping链接超时时间
discovery.zen.ping.multicast.enabled：false 是否打开多播发现节点
discovery.zen.ping.unicast.hosts:["host1","host2:port","host3[portX-portY]"]  设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。
script.engine.groovy.inline.update：on 开启 groovy脚本支持
script.inline：true  开始所有脚本语言行内执行所有支持的操作。

4.3 中文分词器配置
4.3.1 IK分词器安装
索引是把文档写入ES的过程，搜索是匹配查询条件找出文档的过程。第一步是词条化，分词器把输入文本转化为一个个词条流,第二步是过滤，在这个阶段有若干个过滤器处理词条流中的词条。ES中内置了多种分词器可以使用。
Standard Analyzer：标准分词器
Simple Analyzer：简单分词器，基于非字母字符进行分词
Whitespace Analyzer：空格分词器
Stop Analyzer：同简单分词器，增加了停用词过滤功能
Keyword Analyzer：关键词分词器
Pattern Analyzer：正则分词器，单词会被小写，支持停用词
Language Analyzers：特定语言的分词器
Fingerprint Analyzer：指纹分析仪分词器

ES中使用的中文分词器使用最多的是 elasticsearch-analysis-ik，第三方插件， https://github.com/medcl/elasticsearch-analysis-ik 使用版本要与 ES一致

使用：下载后，放入 es的 /plugins/目录中，新建ik文件夹，将解压后的所有文件拷贝到 ik目录中。重启 ES服务

4.3.2 扩展本地词库
可以将扩展词文件放入  xxxx/plugins/ik/config/custom 目录下，新增 hotwords.dic 文件，在其中添加内容
然后在 xxxx/plugins/ik/config/IKAnalyzer.cfg.xml 文件中指定新增的词库位置。
<entry key="ext_dict">
<entry key="ext_stopwords">
<entry key="remote_ext_dict">
<entry key="remote_ext_stopwords">
扩展本地停用词的方法类似，配置完成后 重启 ES。

4.3.3 配置远程词库
在 ik插件的配置文件中指定新增的远程词库地址，然后重启ES，ik会自动重新加载词典。

4.4 Head插件使用指南
4.4.1 Head插件的安装 (参考 oneNode)
4.4.2 Head插件的使用（略）

4.5 REST命令
ES 应用各种任务的 RESTful API  REST的全称是 Representational State Transfer 翻译为：表述性状态转移
特点：
1.采用 客户-服务器 架构，通信只能由单方面发起
2.通信的会话状态由客户端维护
3.响应内容可以在同心链的某处被缓存，改善网络效率
4.通信链组件支架你通过统一的接口相互通信
5.采用分层系统（Layered System）通过限制组件行为，将架构分解为若干等级的层。
6.支持通过下载并执行一些代码，对客户端的功能进行扩展
优点：
高效的利用缓存提高响应速度
通信本身无状态性，可以让不同的服务器处理一系列请求中的不同请求，提高服务器的扩展性。
浏览器可以作为客户端
相对其他叠加在HTTP协议上的机制，REST的软件依赖性更小
不需要额外的资源发现机制
兼容性更好

符合 REST设计风格的 WebAPI 称为 RESTful API，REST是设计风格，不是标准。REST架构中的资源有 URI来指定的。对资源的操作，通过 GET，POST，PUT，DELETE 方法。资源的表现形式可以是 XML，HTML，JSON 或其他的格式

典型应用：
GET 获取，PUT 增加/更新，POST 创建/追加，DELETE 删除

4.5.1 CURL工具
CURL是利用 URL 语法在命令行方式下工作的开源文件传输工具，在Unix,Linux 中，支持多种通信协议。
Ubuntu安装： sudo apt-get install curl libcurl3 libcurl3-dev php5-curl.

4.5.2 Kibana Dev Tools

（参照 OneNode）

第五章 Elasticsearch 集群入门

5.1 索引管理
5.1.1 新建索引
使用 PUT xxxx 语法格式   ，如果索引名称含有大写字母，则会报错
如果添加的索引在 ES中已经存在，同样会报异常
ES默认给一个索引设置5个分片1个副本，分片数设置后不能修改，副本数随时可以修改，可以通过 settings参数设置初始化信息
PUT blog
{
	"settings":{
		"number_of_shards":3,
		"number_of_replicas":0
	}
}

5.1.2 更新副本
PUT blog/_settings
{
	"number_of_replicas":2
}

5.1.3 读写权限
对索引的读写操作进行限制
PUT blog/_settings
{
	"blocks.write":true,   #禁止对索引写操作
	"blocks.read_only":true,   #设置当前索引只允许读，不允许写或者更新
	"blocks.read":true  #禁止对索引读操作
}

5.1.4 查看索引
查看索引的配置信息，在 GET方法上加上 _setting 参数,查看索引的配置信息。
GET test/_settings
还可以同时查看多个索引的 setting信息，GET blog,twitter/_settings
或者查看急群众所有索引的setting信息，GET _all/_settings

5.1.5 删除索引
直接 Delete删除，索引中的文档也将不在。
DELETE blog    #如果删除的索引名不在，将会异常

5.1.6 索引的打开和关闭
一个关闭了的索引，不占系统资源 
POST blog/_close
POST blog/_open   #也可以同时打开或关闭多个索引
对不存在的索引进行 打开或关闭操作，则发生异常，可以使用 ignore_unavailable=true 参数操作只存在的索引
POST blog/_close?ignore_unavailable=true
POST _all/_close  或  POST test*/_close

5.1.7 复制索引
_reindex API 可以把文档从一个索引复制到另一个索引(复制文档信息)，目标索引不会复制源索引的配置信息。
POST _reindex
{
  "source":{"index":"test"},
  "dest": {"index":"test_news"}
}
在复制时，可以增加一些条件限制：
POST _reindex
{
  "source": {
    "index": "blog",
    "type": "article",
    "query": {
      "term": {"title":"git"}
      }
    },
    "dest": {
      "index":"blog_news"
    }
}

5.1.8 收缩索引
索引的分片初始化后无法在做修改，通过 shrink index AP 提供的缩小索引分片数机制，把一个索引变成一个更少分片的索引，收缩后的分片数必须是原始分片数的因子。收缩索引前，索引中的每个分片都要在同一个节点上。
在缩小前，索引必须被标识为制度，所有分片都会复制到一个相同的节点并且健康值为绿色。
PUT test/_settings
{
  "index.routing.allocation.requier._name":"shrink_node_name",
  "index.blocks.write":true
}

POST test/_shrink/test_new
{
  "settings":{
    "index.number_of_replicas":0,
    "index.number_of_shards":1,
    "index.codec":"best_compression"
  },
  "aliases":{
    "my_search_indices":{}
  }
}

5.1.9 索引别名
给索引或多个索引起的另一个名字
POST /_aliases
{
  "actions": [
    {"add": {"index": "test1","alias": "alias1"}}
  ]
}
移除别名：
POST /_aliases
{
  "actions": [
    {"remove": {"index": "test1","alias": "alias1"}}
  ]
}
并且可以同时操作多个索引。还可以有简写形式
POST /_aliases
{
	"actions":[
		{"add":{"indices":["test1","test2"],"alias":"alias1"}}
	]
}
也可以将 增加和删除混合使用
POST /_aliases
{
	"actions":[
		{"remove":{"index":"test1","alias":"alias1"}},
		{"add":{"index":"test1","alias":"alias1"}}
	]
}
说明：如果别名和索引是一对一的，则别名可以直接使用对应的文档。如果别名和索引是一对多的，则使用别名会发生错误。
ES 支持通过通配符同时给多个索引设置别名， 
{"add":{"index":"test*","alias":"alias1"}}

查看索引的别名：
GET /test/_alias

查看所有的别名信息：
GET /_alias

5.2 文档管理
ES中文档的操作和故事逆袭数据库操作非常相似。

5.2.1 新建文档
PUT blog/article/1
{
  "id":1,
  "title":"Git简介",
  "posttime":"2017-05-01",
  "content":"Git是一款免费、开源的分布式版本控制系统"
}
响应的信息中包含创建的文档所在的索引index，类型type，id，版本，分片，是否创建成功。版本号是递增的
如果不指定 id会自动生成，默认是字符串
采用默认方式生成的，必须使用 POST方式，PUT会出现异常

5.2.2 获取文档
使用 GET API 查看在ES中的文档，指定文档所在的索引，类型，id 即可返回一个 JSON文档
GET blog/article/1
在返回值中，found属性表明是否查询到文档，_source字段中是文档的内容

使用 HEAD命令查看一个文档是否存在
HEAD blog/article/1  #返回值巍峨 200 或 404

使用 MULTI GET API来根据索引名，类型名，id，一次获取多个文档，返回一个文档数组

